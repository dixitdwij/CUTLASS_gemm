When editing the TUNING CONFIGURATION SECTION in main:

Threadblock Shape (ShapeTB):

Larger tiles (e.g., 256x128 or 128x256) generally improve data reuse but reduce occupancy (fewer blocks per SM).

Rule of thumb: Try <128, 128, 32> first. Then try <128, 256, 64> if you have large K.

Ensure ThreadblockShape dimensions are multiples of WarpShape.

Warp Shape (ShapeWarp):

Standard is <64, 64, 32>.

Changing this usually has less impact than changing the TB shape, provided it covers the TB shape evenly.

Pipeline Stages:

Hopper has high async copy bandwidth.

Try 3, 4, or 5. Higher stages hide global memory latency better but consume more Shared Memory, potentially reducing occupancy.

K-Dimension:

The "K" part of the tile (e.g., the 32 in <128, 128, 32>) determines the "step" size of the main loop.

Larger K (e.g., 64) improves inner loop efficiency but increases register pressure.

For compile:
srun -A dphpc --time=05:00 --nodelist=studgpu-node09 nvcc dyntuned_gemm.cu -o bin/dyntuned_gemm -I ./lib/cutlass/include/ -I ./lib/cutlass/tools/util/include/ --expt-relaxed-constexpr -arch=sm_90 -std=c++17 -DTB_M=128 -DTB_N=256 -DTB_K=32 -DW_M=64 -DW_N=64 -DW_K=32 -DSTAGES=2 -DFP_32
srun -A dphpc --time=05:00 --nodelist=studgpu-node09 nvcc dyntuned_gemm.cu -o bin/dyntuned_gemm -I ./lib/cutlass/include/ -I ./lib/cutlass/tools/util/include/ --expt-relaxed-constexpr -arch=sm_90 -std=c++17 -DTB_M=128 -DTB_N=256 -DTB_K=32 -DW_M=64 -DW_N=64 -DW_K=32 -DINST_M=16 -DINST_N=8 -DINST_K=16 -DSTAGES=2 -DFP_16 -DVERIFY
srun -A dphpc --time=05:00 --nodelist=studgpu-node09 nvcc dyntuned_gemm.cu -o bin/dyntuned_gemm -I ./lib/cutlass/include/ -I ./lib/cutlass/tools/util/include/ --expt-relaxed-constexpr -arch=sm_90 -std=c++17 -DTB_M=128 -DTB_N=64 -DTB_K=32 -DW_M=64 -DW_N=32 -DW_K=32 -DINST_M=16 -DINST_N=8 -DINST_K=16 -DSTAGES=4 -DFP_16 -DVERIFY

For running:
srun -A dphpc --time=01:00 --nodelist=studgpu-node09 ./bin/dyntuned_gemm 8192 8192 512

git commit -m 'Added support for fp32, fp16, bf16 as compile time params'